{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Regions Workshop OHBM 2023\n",
    "## Beyond Blobology: Advances in Statistical Inference for Neuroimaging\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Confidence Regions Workshop at OHBM 2023! This workshop will introduce you to the concept of Confidence Regions (CRs) and how they can be used to make inferences about brain activity. The session shall cover the generation of CRs for excursion sets derived from %BOLD maps, and demonstrate CRs using a range of examples. By the end of this workshop, you will have a better understanding of the following:\n",
    "\n",
    "- What CRs are, both conceptually and mathematically.\n",
    "- When you may want to use CRs, and why.\n",
    "- The factors that influence the size and shape of CRs.\n",
    "- Common pitfalls of CR interepretation.\n",
    "- What software is available to generate CRs, and how to use it.\n",
    "\n",
    "In this notebook, we will discuss CRs in the context of the recent debate about the use of null-hypothesis significance testing (NHST) in neuroimaging, and explain how CRs can be used to complement NHST. We shall also show how CRs can be generated for effect size images, as well as BOLD maps.\n",
    "\n",
    "If you have any questions during this session, please feel free to ask me in person at any time, or reach out via email at [TomMaullin@gmail.com](mailto:TomMaullin@gmail.com). An anonymous feedback form is also available [here](https://forms.gle/n5yZ4N9UdQnCVCnJ8)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    " - [Installation](#installation)\n",
    " - [Background](#background)\n",
    " - [Why use Confidence Regions? Confidence Regions vs Null Hypothesis Testing](#why-use-confidence-regions-confidence-regions-vs-null-hypothesis-testing)\n",
    " - [What are Confidence Regions?](#what-are-confidence-regions)\n",
    " - [What are Confidence Regions Not?](#what-are-confidence-regions-not)\n",
    " - [Under the Hood: How are Confidence Regions Generated?](#under-the-hood-how-are-confidence-regions-generated)\n",
    " - [2D Example](#2d-example)\n",
    " - [3D fMRI Simulated Example](#3d-fmri-simulated-example)\n",
    " - [3D fMRI Real Data Example](#3d-fmri-real-data-example)\n",
    " - [Cohen's d Confidence Regions](#cohens-d-confidence-regions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install requests\n",
    "%pip install crtoolbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Imports from Confidence Regions Toolbox\n",
    "from crtoolbox.generate import generate_CRs\n",
    "\n",
    "# Import supporting functions\n",
    "from crtoolbox.lib.set_theory import *\n",
    "from crtoolbox.lib.boundary import *\n",
    "from crtoolbox.lib.regression import *\n",
    "from crtoolbox.lib.cohens import *\n",
    "\n",
    "# Import data generation\n",
    "from crtoolbox.tests.generate_2d_data import *\n",
    "from crtoolbox.tests.generate_ni_data import *\n",
    "\n",
    "# Import plotting functions\n",
    "from crtoolbox.lib.display import display_crs, display_volume\n",
    "\n",
    "# Import data download function\n",
    "from data.download import download_and_extract_zip_from_dropbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In this notebook, we will start by considering a *signal plus noise* model of the form:\n",
    "\n",
    "$$Y(s)=\\mu(s)+\\epsilon(s), \\quad \\epsilon(s)\\sim N(0,\\sigma^2(s)I_n)$$\n",
    "\n",
    "Here $s$ is a spatial location (e.g. an $(x,y,z)$ coordinate for a point in the brain), $\\mu$ is the signal we are interested in estimating (e.g. %BOLD response), $Y$ is our data, and $\\epsilon$ is the random noise. Many conventional fMRI analyses fall into this framework, most notably a standard group-level linear regression, which we shall consider later in this notebook. Our interest lies in estimating the *excursion set* $\\mathcal{A}_c$, defined as:\n",
    "\n",
    "$$\\mathcal{A}_c:=\\{s: \\mu(s)\\geq c\\}$$\n",
    "\n",
    "Here $c$ is a predefined threshold (for instance, $c= 1$\\%BOLD change). In the context of fMRI group-level analysis, this set may represent the regions of the brain for which \\%BOLD response exceeded a predefined amount. We will be investigating this set using Confidence Regions, which will be denoted $\\hat{\\mathcal{A}}_c^{+}$ and $\\hat{\\mathcal{A}}_c^{-}$. \n",
    "<div>\n",
    "<img src=\"./assets/figs/excursion_sets.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** We assume that we have i.i.d. observations for each subject, but we do not make any assumptions on the *spatial covariance* of our images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an estimate of $\\mu$, denoted $\\hat{\\mu}$ and derived from $Y$, our aim is to assess how reliable our estimate of the excursion set, $\\mathcal{A}_c$, is. Here, our estimate, $\\hat{\\mathcal{A}}_c$ is given by:\n",
    "\n",
    "$$\\hat{\\mathcal{A}}_c=\\{s:\\hat{\\mu}(s)\\geq c\\}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In the following sections, both $\\mathcal{A}_c$ and $\\hat{\\mathcal{A}}_c$ will be displayed in yellow in figures. Make sure to double check which set you are looking at in the following figures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this more concrete, let's look at some example data. For demonstrative purposes, this data is simulated, but it is designed to roughly resemble what might be seen in reality. Here is the simulated %BOLD response for a single subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in some example data\n",
    "Y_instance = os.path.join(os.getcwd(),'data','example1','Data_Instance_Example1.nii.gz')\n",
    "\n",
    "# Display the Y instance using the display_volume function\n",
    "display_volume(Y_instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Developer's Note:** If you can only see an empty display with a slider in your Jupyter notebook, try clicking on the slider to see if anything changes. If this doesn't work for you, or the slider isn't appearing at all, try setting `display` to `simple` and enter the `slice` you wish to view as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_volume(Y_instance, display='simple', slice=60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Developer's note:** Viewing NIFTI images can be computationally intensive. For the best performance when running this notebook, we recommend never having too many interactive NIFTI displays open at once. Once you have had a look at a NIFTI in interactive mode, it may be best to choose a slice you like and change the display to a simple display for that slice before moving on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model we gave above, if $s$ is the location of voxel, $Y(s)$ would be the $(n \\times 1)$ vector obtained by stacking together the values observed at $s$ in each of the $n$ observed images.\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/figs/bold_y.png\" width=\"700\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the signal $\\mu$ used to generate this data. $\\mu$ consists of a single large cluster of activation, somewhere inside the parietal lobe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in some mu\n",
    "mu_file = os.path.join(os.getcwd(),'data','example1','True_Signal_Example1.nii.gz')\n",
    "\n",
    "# We'll add a mask as a background to help visualise the data\n",
    "mask_file = os.path.join(os.getcwd(),'data','mask.nii.gz')\n",
    "\n",
    "# Display mu using the display_volume function\n",
    "display_volume(mu_file, bg = mask_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be aiming to estimate the region where $\\mu > c$. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our binary set we wish to estimate\n",
    "Ac_file = os.path.join(os.getcwd(),'data','example1','True_Ac_Example1.nii.gz')\n",
    "\n",
    "# We'll add a mask as a background to help visualise the data\n",
    "mask_file = os.path.join(os.getcwd(),'data','mask.nii.gz')\n",
    "\n",
    "# Display mu using the display_volume function\n",
    "display_volume(Ac_file, bg = mask_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few sections, we shall use the above mock example to demonstrate some basic properties of CRs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Confidence Regions? Confidence Regions vs Null Hypothesis Testing\n",
    "\n",
    "Before going further, it is worth briefly discussing why you may be interested in using CRs instead of, say, conventional Null Hypothesis Significance Testing. NHST is a staple of fMRI anlaysis. However, it is not without it's problems. These include, for instance:\n",
    "\n",
    "- The *\"Null hypothesis fallacy\"*; the problem that even trivial effects can be determined as significant when testing $H_0:\\mu=0$.\n",
    "\n",
    "- Significance values are arbitrarily chosen but crucial to the analysis interpretation (*\"Surely, God loves the .06 nearly as much as the .05.\" Robert Rosenthal, 1979* [[1]](https://psycnet.apa.org/record/1979-27602-001)).\n",
    "\n",
    "- Observing that an effect is non-zero effect does not provide any information about the size of the effect. \n",
    "\n",
    "- Studies can fail to reject the null hypothesis due to limited statistical power rather than evidence in favour of the null hypothesis.\n",
    "\n",
    "These criticims have been repeatedly raised in the statistics literature for over 70 years, and are still applicable to many of the fMRI analyses conducted today. A good summary of the prevailing viewpoint is given by the American clinical psychologist Paul E. Meehl [[2]](https://meehl.umn.edu/sites/meehl.umn.edu/files/files/169problemisepistemology.pdf): \n",
    "\n",
    "> *\"The Problem Is Epistemology, Not Statistics: Replace Significance Tests by Confidence Intervals and Quantify Accuracy of Risky Numerical Predictions\"*\n",
    "\n",
    "The same comment applies in higher dimensions; *\"Replace (or at least complement) Significance Tests with Confidence Intervals and Effect Sizes\"*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Confidence Regions?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, CRs are similar to confidence intervals but, instead of upper and lower bounds for a scalar parameter, they provide estimated inner and outer sets, $\\hat{\\mathcal{A}}_c^{+}$ and $\\hat{\\mathcal{A}}_c^{-}$, to bound an excursion set $\\mathcal{A}_c$. When we see wide confidence intervals for a scalar parameter estimate, this suggests that the procedure which produced the estimate is not very reliable.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/figs/confidence_intervals.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Similarly, when we see CRs which exhibit little resemblance to one another, this is typically interpreted as a lack of reliability in the estimation of the excursion set. In other words, tight resemblance means the procedure used to estimate the excursion set is realiable.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"./assets/figs/confidence_regions.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, for a tolerance level $\\alpha$, CRs are sets designed satisfy the following expression:\n",
    "\n",
    "$$\\mathbb{P}[\\hat{\\mathcal{A}}^{+}_c\\subseteq\\mathcal{A}_c\\subseteq\\hat{\\mathcal{A}}^{-}_c]=1-\\alpha \\quad \\quad \\quad (1)$$\n",
    "\n",
    "Conventional tolerance levels tend to include $\\alpha=0.1,0.05$ and $0.01$ ($10$\\%, $5$\\% and $1$\\%). Throughout this notebook, unless stated otherwise we shall used $\\alpha=5\\%$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying CRs, we will always show the inner set ($\\hat{\\mathcal{A}}_c^{+}$) in red, the outer set ($\\hat{\\mathcal{A}}_c^{-}$) in blue and the point estimate ($\\hat{\\mathcal{A}}_c$) in yellow. For the CRs we will be generating, it follows from the construction we will use that the sets $\\hat{\\mathcal{A}}_c^{+}$, $\\hat{\\mathcal{A}}_c$ and $\\hat{\\mathcal{A}}_c^{-}$ will always be nested within one another (as opposed to the set $\\mathcal{A}_c$ which will lie between $\\hat{\\mathcal{A}}_c^{+}$ and $\\hat{\\mathcal{A}}_c^{-}$ in $(1-\\alpha)$\\% of future experiments)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in some pre-computed CRs to see what this might look like for fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pre-computed confidence regions.\n",
    "Upper_CR_fname = os.path.join(os.getcwd(),'data','example1','Upper_CR_Example1.nii.gz')\n",
    "Lower_CR_fname = os.path.join(os.getcwd(),'data','example1','Lower_CR_Example1.nii.gz')\n",
    "\n",
    "# Load in a pre-computed point estimate of Ac.\n",
    "estimated_Ac_fname = os.path.join(os.getcwd(),'data','example1','Estimated_Ac_Example1.nii.gz')\n",
    "\n",
    "# Load in a pre-computed mask.\n",
    "mask_fname = os.path.join(os.getcwd(),'data','mask.nii.gz')\n",
    "\n",
    "# Display confidence regions in interactive slice plot.\n",
    "display_crs(estimated_Ac_fname, Upper_CR_fname, Lower_CR_fname, mask_fname, mode='Sagittal')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Developer's Note:** As with the `display_volume` function, if you can only see an empty display with a slider in your Jupyter notebook, try clicking on the slider to see if anything changes. You can also `display` to `simple` and enter the `slice` you wish to view as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confidence regions in non-interactive slice plot.\n",
    "display_crs(estimated_Ac_fname, Upper_CR_fname, Lower_CR_fname, mask_fname, mode='Sagittal', display='simple', slice=60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These CR's were generated using $B=5000$ bootstrap instances. By the end of this notebook, you will have made your own CRs using both 2D and 3D synthetic data, as well as for a real-data example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Test your understanding:** The images we have looked at so far can all be found in the `./data/example1` folder. Another set of CRs can be found in `./data/example2` folder under similar filenames. Using the functions described above, have a look at these files. If these were your analysis results, which CRs do you think would be preferable to observe? And why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Confidence Regions **not**?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with confidence intervals, it is very easy to misinterpet the definition of Confidence Regions. A common reason for misinterpretation is confusion over which sets are random and which are fixed. Here are some common **incorrect interpretations** of CRs:\n",
    "\n",
    " - If we generate a pair of 95\\% CRs $\\hat{\\mathcal{A}}^{+}_c$ and $\\hat{\\mathcal{A}}^{-}_c$, this  **does not** mean that there is a 95\\% probability $\\mathcal{A}_c$ is nested between the particular CRs that we have generated. This is because $\\mathcal{A}_c$ is a fixed (non-random) set, and once we know the values of $\\hat{\\mathcal{A}}^{+}_c$ and $\\hat{\\mathcal{A}}^{-}_c$, they also are fixed. This means that, for any given pair of CRs, the probability of $\\mathcal{A}_c$ lying between them must be zero or one. \n",
    " - A pair of 95\\% CRs **do not** 'contain' the excursion set for individual subjects (e.g. $\\{s: Y_k(s)\\geq c\\}$ for subject $k$) 95\\% of the time.\n",
    " - A given pair of 95\\% CRs, generated from a single run of the method, **will not** 'contain' the estimated excursion set $\\hat{\\mathcal{A}}_c$ in 95\\% of replications.\n",
    "\n",
    "Here are some **correct interpretations**:\n",
    "\n",
    " - If the 95\\% CR generating procedure were repeated on numerous samples, the proportion of CRs, $\\hat{\\mathcal{A}}^{+}_c$ and $\\hat{\\mathcal{A}}^{-}_c$, that correctly enclose $\\mathcal{A}_c$ would tend to 95\\%.\n",
    " - There is a 95\\% probability that the region $(\\hat{\\mathcal{A}}^{+}_c)^c \\cap \\hat{\\mathcal{A}}^{-}_c$, in a future experiment, fully encompasses the boundary of $\\mathcal{A}_c$.\n",
    " - In the fMRI context, within a 95\\% upper CR (red set) we can assert with 95\\% confidence that there has been (at least) a c\\% change in BOLD response. Similarly, within the 95\\% lower CR (blue set), we can assert with 95\\% confidence that there has been (at most) a c\\% change in BOLD response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood: How are Confidence Regions Generated?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is designed to be practical, rather than mathematical. However, for those interested in understanding how we generate CRs this section provides a brief overview. If you are not interested in the math, this section can be safely skipped and you should feel free to move ahead to 2D example.*\n",
    "\n",
    "Many authors have suggested methods for generating CRs which satisfy Equation $(1)$. Here are some very brief summaries of commonly employed CR generation methods:\n",
    "\n",
    "- Empirically estimate the distribution of the [Hausdorff distance](https://en.wikipedia.org/wiki/Hausdorff_distance) between $\\mathcal{A}_c$ and $\\hat{\\mathcal{A}}_c$ and then generate CRs by 'fattening' $\\hat{\\mathcal{A}}_c$ using the $(1-\\alpha)$ quantile of this distribution (see [[3]](https://doi.org/10.1080/01621459.2016.1228536) for further detail).\n",
    "- Represent the excursion sets using a parametric family and apply a sequential importance sampling method for estimating joint probabilities (see [[4]](http://www.jstor.org/stable/24774726)). \n",
    "- Using properties of the noise process along the boundary of $\\mathcal{A}_c$, assess the probability of $\\hat{\\mathcal{A}}^{+}_c\\subseteq\\mathcal{A}_c\\subseteq\\hat{\\mathcal{A}}^{-}_c$ holding (see [[5]](https://doi.org/10.1080/01621459.2017.1341838)).\n",
    "\n",
    "The latter method is our favoured approach, and the one which is employed throughout this notebook, so we will explain it in a little more detail. This approach was originally described in the paper [\"Confidence Regions for Spatial Excursion Sets From Repeated Random Field Observations, With an Application to Climate (2017)\"]((https://doi.org/10.1080/01621459.2017.1341838)), which we shall refer to as SSS for short after the second initials of the authors; Max Sommerfeld, Stephan Sain & Armin Schwartzman. \n",
    "\n",
    "In SSS, the spatial domain (e.g. the brain in fMRI) we are working in is denoted as $S$, and a functional central limit theorem of the following form is assumed:\n",
    "\\begin{equation}\\nonumber\n",
    "\\bigg\\{\\frac{\\hat{\\mu}(s) - \\mu(s)}{\\sigma(s)/\\sqrt{n}}\\bigg\\}_{s \\in S} \\to \\big\\{G(s)\\big\\}_{s \\in S} \\quad \\text{as }\\quad n \\rightarrow \\infty.\n",
    "\\end{equation}\n",
    "Here, $G$ is a well-defined random field on $S$ with almost surely continuous sample paths. The notation $G$ is indicative of the fact that in many practical instances, $G$ is a Gaussian random field with mean zero and unit pointwise variance (with no assumed spatial covariance). To generate CRs, however, we do not require any such distributional assumptions.\n",
    "\n",
    "SSS then assumes that CRs are constructed as follows:\n",
    "\\begin{equation}\\nonumber\n",
    "    \\hat{\\mathcal{A}}_c^{\\pm}=\\bigg\\{s \\in S : \\frac{\\hat{\\mu}(s)-c}{\\hat{\\sigma}(s)/\\sqrt{n}} \\geq \\pm q\\bigg\\}\n",
    "\\end{equation}\n",
    "for some quantile $q \\in \\mathbb{R}$ which must be estimated empirically. In SSS, it was shown, under minor assumptions which guarantee that the topology of $\\mu$ does not change at the level $c$, that:\n",
    "\\begin{equation}\\nonumber\n",
    "\\lim_{n \\rightarrow \\infty}\\mathbb{P}[\\hat{\\mathcal{A}}^{+}_c\\subseteq\\mathcal{A}_c\\subseteq\\hat{\\mathcal{A}}^{-}_c]=\\mathbb{P}\\bigg[\\sup_{s \\in \\partial \\mathcal{A}_c} |G(s)|\\leq q \\bigg] \\quad \\quad (2)\n",
    "\\end{equation}\n",
    "where $\\partial \\mathcal{A}_c$ is the boundary of $\\mathcal{A}_c$, assumed to be equal to the level set $\\{s \\in S: \\mu(s)=c\\}$.\n",
    "\n",
    "> **Developer's note:** You can create images of $\\partial\\mathcal{A}_c$ in the CR toolbox using the `get_bdry_map_combined` function, shown below. It is worth clarifying that we only use the boundary 'inside' the brain mask (see slice 60) - the edge of the brain mask does not count as part of $\\partial\\mathcal{A}_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in true Ac\n",
    "true_Ac_fname = os.path.join(os.getcwd(),'data','example1','True_Ac_Example1.nii.gz')\n",
    "\n",
    "# Generate boundary map\n",
    "Ac_bdry = get_bdry_map_combined(true_Ac_fname, mask=mask_fname)\n",
    "\n",
    "# Display boundary map\n",
    "display_volume(Ac_bdry, bg = mask_fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important point to note here is that, to generate CRs which satisfy Equation $(1)$ asymptotically, we only need to find a value $q$ such that the right hand side of $(2)$ equals $(1-\\alpha)\\%$. In other words, we need to find the $(1-\\alpha)\\%$ quantile of the distribution of $\\sup_{\\partial \\mathcal{A}_c} |G(s)|$. To find this quantile, we perform a wild $t$-bootstrap procedure proposed by [[6]](https://doi.org/10.1016/j.neuroimage.2019.116187) using the standardized residual images from a linear regression.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- SSS CRs are generated as excursion sets of $\\frac{\\hat{\\mu}(s)-c}{\\hat{\\sigma}(s)/\\sqrt{n}}$ using a quantile $q$.\n",
    "- The probability that SSS CRs are correctly nested is asymptotically equal to the probability that $\\sup_{\\partial \\mathcal{A}_c} |G(s)|\\leq q$.\n",
    "- We can choose $q$ to satisfy $\\mathbb{P}[\\sup_{\\partial \\mathcal{A}_c} |G(s)| \\leq q]=1-\\alpha$ by performing a Wild-$t$ bootstrap using the standardized residuals from a regression.\n",
    "\n",
    "To make things more concrete, let's now turn to some examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CR generation requires boostrapping, which is a time and memory consuming process. Before we look at fMRI analyses with large images containing hundreds of thousands of voxels, let's first use a toy example with 2D images to investigate some features of confidence regions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by creating a toy dataset of 100 2D images. To begin, let's create a 2D signal, $\\mu$. We have prepared three example $\\mu$ signals you can use in this section; a circle, square and ramp signal. Here is an example of a circular signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will define a circular smooth signal\n",
    "circle_signal = CircleSignal(r=30, fwhm=[10,10], mag=1)\n",
    "\n",
    "# Once we have defined a signal we can plot the output like so:\n",
    "circle_signal.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the following properties when defining your circle signal:\n",
    "\n",
    " - `r`: This is the radius of the circle.\n",
    " - `mag`: This is the magnitude of the signal (signal height).\n",
    " - `fwhm`: The full width half maximum representing the smoothness of the image (this must be a list containing a smoothness value for each dimension e.g. `[4,5]`).\n",
    " - `center`: This is where the circle is centered with `[0,0]` representing the center of the image (this must be a list containing a coordinate for each dimension e.g. `[10,10]`).\n",
    " - `dim`: This is the image dimensions (by default `[100,100]`). As we may generate a lot of images, it would be advisable not to make these dimensions larger than this for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a square signal as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will define a smooth square signal\n",
    "square_signal = SquareSignal(r=20)\n",
    "\n",
    "# Once we have defined a signal we can plot the output like so:\n",
    "square_signal.plot()        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options for square signal generation are identical to those of a circle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a linear ramp signal you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command will define a linear ramp signal\n",
    "ramp_signal = RampSignal(a=1.5,b=4)\n",
    "\n",
    "# Once we have defined a signal we can plot the output like so:\n",
    "ramp_signal.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a ramp you have the following options:\n",
    "\n",
    "- `a`: The magnitude at the lower end of the ramp.\n",
    "- `b`: The magnitude at the higher end of the ramp.\n",
    "- `orient`: The orientation of the ramp (`vertical` or `horizontal`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one of the signal functions above, choose a signal that you want to use for this section below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_signal = # Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've decided on the $\\mu$, we need to define the noise field, $\\epsilon$. To do this, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the noise\n",
    "noise = Noise(var=5,fwhm=[5,5])\n",
    "\n",
    "# Plot a single realisation of the noise\n",
    "noise.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To generate noise you can use the following options:\n",
    "- `type`: Type of noise to generate. Can be `homogen` (homogeneous) or `heterogen` (a heterogenous linear ramp).\n",
    "- `fwhm`: fwhm of smoothing kernel.\n",
    "- `var`: Variance of noise.\n",
    "- `dim`: Dimensions of image.\n",
    "- `n`: Number of observations to generate (default is set to `100`).\n",
    "\n",
    "Choose the noise you want to use for this section below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_noise = # Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these objects, we shall now generate some test data to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory (feel free to change this to your desired output directory)\n",
    "out_dir = os.path.join(os.getcwd(),'data','example_2D')\n",
    "\n",
    "# We can generate some 2D data using the generate_data_2D function\n",
    "data_files, mu_file = generate_data_2D(my_signal, my_noise, out_dir=out_dir)\n",
    "\n",
    "# Let's record the number of observations; this will be useful later\n",
    "n = len(data_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at these files using the same `display_volume` function as before, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualise the data using the functions we have already seen.\n",
    "display_volume(data_files[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've succesfully generated some data. Our aim now is to estimate the excursion set $\\mathcal{A}_c$, and generate confidence regions for it. As this is a simulation, we know $\\mu$ and can look at the true excursion set, $\\mathcal{A}_c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a threshold c\n",
    "c = 2\n",
    "\n",
    "# Read mu from file\n",
    "mu = read_image(mu_file)\n",
    "\n",
    "# Create boolean map of Ac\n",
    "Ac = mu>c\n",
    "\n",
    "# Display Ac\n",
    "display_volume(Ac)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate confidence regions, we will need a mean estimate ($\\hat{\\mu}$), a standard deviation estimate ($\\hat{\\sigma}$) and a residual image for each subject. We can do this using the `regression` function from the confidence regions toolbox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a design matrix with only an intercept (this means that the parameter estimate from the regression will be the mean of the data)\n",
    "X = np.ones((n,1))\n",
    "\n",
    "# Fit the regression model\n",
    "muhat_file, sigma_file, resid_files = regression(data_files, X, out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the mean estimate $\\hat{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated mu\n",
    "display_volume(muhat_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimated excursion set, $\\hat{\\mathcal{A}}_c$, is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the binary map for the estimated Ac\n",
    "Ac_hat = read_image(muhat_file) > c\n",
    "\n",
    "# Display the estimated excursion set\n",
    "display_volume(Ac_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we have now generated some simulated data, obtained a group average and estimated our \"region of activation\". If this were a typical group-level fMRI analysis, to tell if this region was meaningful in some statistical sense, we would likely perform a T-test of some form. Here, we are going to instead compute confidence regions. We can do this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide a confidence level alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# Generate confidence regions (to save time, we will use 2000 bootstrap iterations for now)\n",
    "lower_cr_file, upper_cr_file, estimated_ac_file, quantile_estimate = generate_CRs(muhat_file, sigma_file, resid_files, out_dir, c, 1-alpha, n_boot=2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of the `generateCRs` file are:\n",
    "\n",
    "- `lower_cr_file:` The filename of the file containing the lower confidence region, , $\\hat{\\mathcal{A}}^-_c$.\n",
    "- `upper_cr_file:` The filename of the file containing the upper confidence region, $\\hat{\\mathcal{A}}^+_c$.\n",
    "- `estimated_ac_file:` The filename of the file containing the estimated excursion set, $\\hat{\\mathcal{A}}_c$.\n",
    "- `quantile_estimate:` The estimated quantile of the supremum probability given in Equation $(2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated mu\n",
    "display_crs(estimated_ac_file, upper_cr_file, lower_cr_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Test your understanding:** Congratulations, you've made some CRs! Now you have generated one set of CRs, try going back to signal and noise generation and test a few different options. What do you think impacts the observed CR 'resemblance'? Which variables make the CRs thinner or fatter? For instance, what are the effects of changing sample size, noise variance, smoothness and so forth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have it in memory, let's have a quick look at the quantile estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of quantile estimates\n",
    "print(quantile_estimate[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Test your understanding:** If you are interested in the math behind CRs, see if you can guess what `quantile_estimate` is in the notation of the `Under the hood: How are CRs generated?` section. If you are unsure, feel free to ask."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `generate_crs` method to return more quantiles of the distribution of $\\sup_{\\partial\\mathcal{A}_c}|G(s)|$. This is done by specifying an array of `alpha` values like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alpha values\n",
    "alphas = np.linspace(0,1,51)\n",
    "\n",
    "# Generate quantiles (for time purposes we will only use 2000 bootstrap iterations for now)\n",
    "_, _, _, quantile_estimates = generate_CRs(muhat_file, sigma_file, resid_files, out_dir, c, 1-alphas, n_boot=5000, output=False)\n",
    "\n",
    "# Convert the CDF to a PDF (this is just done by taking the gradient of the CDF)\n",
    "pdf = np.gradient(1-alphas, quantile_estimates)\n",
    "\n",
    "# Plot the PDF of sup|G|\n",
    "plt.plot(quantile_estimates, pdf)\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('sup|G|')\n",
    "plt.ylabel('Probability Distribution Function')\n",
    "\n",
    "plt.title('Empirical PDF of sup|G|')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In the above code, we set `output` to false to prevent CRs being output for every value of $\\alpha$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to move to fMRI examples and regression. Before moving on, here are some final quick notes on CRs:\n",
    "\n",
    "- Regression in the CR toolbox can be performed using the `regression` function.\n",
    "\n",
    "- CRs can be generated using the `generate_crs` function.\n",
    "\n",
    "- The 'tightness' of CR resemblance is influenced by a range of factors, including:\n",
    "\n",
    "    - Sample size: Small samples result in more variable estimates.\n",
    "\n",
    "    - Smoothness: If $\\mu$ is very smooth, then small amounts of noise can have a strong impact on the shape of $\\mathcal{A}_c$.\n",
    "\n",
    "    - Excursion set size: If there is more to estimate, there is more room for error.\n",
    "\n",
    "    - Noise magnitude: If there is larger noise, it will be harder to estimate the signal.\n",
    "\n",
    "    - Noise heterogeneity: If the noise is larger in one part of an image than another, the CRs will reflect this.\n",
    "\n",
    "\n",
    "\n",
    "- The performance of the CR method is robust to a range of factors including: \n",
    "\n",
    "    - The smoothness levels used.\n",
    "    \n",
    "    - Excursion set shape and size.\n",
    "    \n",
    "    - Choice of threshold $c$ (provided the $A_c$ is not a peak, minima or plateau).\n",
    "    \n",
    "    - Realistic variation in noise magnitude and heterogeneity in the noise.\n",
    "    \n",
    "    - For small samples, however, some overcoverage may be observed.\n",
    "\n",
    "> **Developer's Note:** If you wish to investigate the performance of CRs yourself (i.e. if you want to check how accurately the CRs actually obtain 1-$\\alpha$ coverage), have a look in `./assets/extra.py` for some custom code. But note that performance assessment can be time intensive, so only do this if you finish the notebook early."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to clean up the files generated from this section, please use the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files or file collections to remove\n",
    "files_to_remove = [data_files, resid_files, mu_file, muhat_file, sigma_file, lower_cr_file, upper_cr_file, estimated_ac_file]\n",
    "\n",
    "# Remove each file or file collection\n",
    "for files in files_to_remove:\n",
    "    remove_files(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D fMRI Simulated Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have investigated the behaviour of CRs on small 2D examples, we're ready to move to 3 dimensions! In this section, we will generate CRs for a simulated group-level fMRI analysis. \n",
    "\n",
    "To begin, let's generate some data.\n",
    "\n",
    "> **Warning:** In this section, we will be generating nifti images, which are much larger than the `.npy` files we used for the 2D example. If you have limited space on your computer, please consider setting the sample size, `n`, (e.g. the number of images we are generating) and number of parameters, `p`, (e.g. the number of regression coefficients we are estimating) to small values in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subjects\n",
    "n = 30\n",
    "\n",
    "# Number of parameters\n",
    "p = 3\n",
    "\n",
    "# Output directory (feel free to change this to your desired output directory)\n",
    "out_dir = os.path.join(os.getcwd(),'data','example_3D')\n",
    "\n",
    "# Generate some test data\n",
    "y_files, beta_files, X = generate_data(n, p, out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just generated some synthetic data according to the linear model:\n",
    "\n",
    "\\begin{equation}\\nonumber\n",
    "    Y(s) = X\\beta(s) + \\epsilon(s), \\quad \\epsilon(s)\\sim N(0,\\sigma^2(s)I_n)\n",
    "\\end{equation}\n",
    "\n",
    "In this model $X$ is an $(n \\times p)$ design matrix containing an intercept and $p-1$ uniformly generated mean zero columns and, for each spatial location (voxel) $s$: \n",
    "- $Y(s)$ is the $(n \\times 1)$ response vector.\n",
    "- $\\beta(s)$ is the $(p \\times 1)$ parameter vector.\n",
    "- $\\epsilon(s)$ is an $(n \\times 1)$ random error vector.\n",
    "- $\\sigma^2(s)$ is a scalar error variance.\n",
    "\n",
    "This simulated data is designed to resemble something like the group-level linear model illustrated below:\n",
    "<div>\n",
    "<img src=\"./assets/figs/glm2_nb.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Here, the signal we are interested in estimating is $\\mu:=L'\\beta$, where $L$ is a fixed and known $(1 \\times p)$-sized contrast vector specifying some belief about the linear relationships between the elements of $\\beta$. To keep things simple, in this example we will just be using $L=[1,0,...,0]$, meaning that $\\mu=\\beta_0$, the $\\beta$ image corresponding to the intercept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at some of the data we have just made. Here's an instance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first Y image\n",
    "display_volume(y_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the $\\beta$ map we will be investigating. This was generated using a circular signal of a random radius, placed randomly somewhere near the center of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask file for background\n",
    "mask_file = os.path.join(os.getcwd(),'data','mask.nii.gz')\n",
    "\n",
    "# Display the beta image we will be estimating\n",
    "display_volume(beta_files[0], bg=mask_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a regression to get the images we need. You can use the same `regression` function as in the 2D example to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betahat_files, sigma_file, resid_files = # Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save memory, you can now delete the `y_files` using the below code if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean\n",
    "you_would_like = True\n",
    "\n",
    "# Delete files if you would like\n",
    "if you_would_like:\n",
    "\n",
    "    # Remove data files\n",
    "    remove_files(y_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at our signal estimate, $\\hat{\\beta}_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated beta image\n",
    "display_volume(betahat_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to generate some 3D CRs! We can do this as follows: \n",
    "\n",
    "> **Developer's Note:** As the 3D images are 100 times bigger than the 2D images, this might take a little more time to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide a confidence level alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# To save time, we suggest using 2000 bootstrap iterations at first\n",
    "n_boot = 2000\n",
    "\n",
    "# Threshold c\n",
    "c = 2\n",
    "\n",
    "# Generate confidence regions\n",
    "lower_cr_file, upper_cr_file, estimated_ac_file, quantile_estimate = generate_CRs(betahat_files[0], sigma_file, resid_files, out_dir, c, 1-alpha, n_boot=n_boot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at these CRs by writing some code in the box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Test your understanding:** Compare these to the $\\beta$ estimate we displayed above. What do these CRs tell you about the estimate?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now know all you need to generate CRs in practice. In the next section, you will use what you have learnt to generate CRs for a real-world dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to remove the files from this section set this to True\n",
    "you_would_like = True\n",
    "\n",
    "# Delete files if you would like\n",
    "if you_would_like:\n",
    "\n",
    "    # List of files or file collections to remove\n",
    "    files_to_remove = [y_files, resid_files, betahat_files, beta_files, sigma_file, lower_cr_file, upper_cr_file, estimated_ac_file]\n",
    "\n",
    "    # Remove each file or file collection\n",
    "    for files in files_to_remove:\n",
    "        remove_files(files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D fMRI Real Data Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, it is up to you! In the following few boxes, we will talk you through downloading some real-world data, lifted from the [Wu-Minn HCP dataset](https://www.humanconnectome.org/study/hcp-young-adult/document/wu-minn-hcp-consortium-open-access-data-use-terms)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** To participate in this section, you **MUST** sign the 'Data Use Terms'. Please follow the instructions given in [HCP_Access_Form_Instructions.pdf](./HCP_Access_Form_Instructions.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download the images. If you participated in the 'TDP Inference in regression' class this morning, you do not need to download the data again. Instead you can just change the below `data_dir` folder to the location you downloaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data directory\n",
    "real_data_dir = os.path.join(os.getcwd(),'data','example_real_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide Dropbox file URL and Local system path where file needs to be downloaded and extracted.\n",
    "bold_files, covariates = download_and_extract_zip_from_dropbox(real_data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was derived from the Wu-Minn HCP working memory task. Here is some further detail on the task itself.\n",
    "\n",
    "**The Task**\n",
    "\n",
    "In the experiment, 77 subjects performed two tasks spread across two runs where each run contained four blocks. During each block, the subject undertook either a 2-back memory task or a 0-back control task. The experimental design was arranged such that, in each run, two blocks were designated to the 2-back memory task, and two blocks were designated to the 0-back control task. In each block a subject was shown a stimuli image (a picture of a face or a place, for instance) and then asked to recall the image they were shown. They were either asked to recall the most recent image (the '0-back' image) or the image shown to them two images prior (the '2-back' image). Interest lies in assessing whether this delay impacted the \\%BOLD response.\n",
    "\n",
    "**First-level Analysis**\n",
    "\n",
    "In FSL, a first-level analysis has been conducted independently for each subject. In each first-level analysis, the task design was regressed onto Blood Oxygenation Level Dependent (BOLD) response and a Contrast Parameter Estimate (COPE) map was generated. Each COPE map represents, for a given subject, the difference in BOLD response between the subject performing the 2-back task and the 0-back task (e.g $L\\hat{\\beta}=\\hat{\\beta}_{\\text{2-Back}}-\\hat{\\beta}_{\\text{0-Back}})$).\n",
    "\n",
    "**Acquisition Details**\n",
    "\n",
    "All image acquisitions were obtained using a $32$ channel head coil on a modified $3T$ Siemans Skyra scanner via a gradient-echo EPI sequence with TR $=720$ ms, TE $=33.1$ ms, $2.0$ mm slice thickness, $72$ slices, $2.0$ mm isotropic voxels, $208$ mm $\\times 180$ mm FOV, and a multi-band acceleration factor of $8$.\n",
    "\n",
    "**The Data**\n",
    "\n",
    "You have the COPE map for each subject and the following covariates for each subject, both sorted by subject ID.\n",
    " - `Age`: The subject's age.\n",
    " - `Sex`: The subject's biological sex, with male encoded as `0` and female encoded as `1`.\n",
    " - `PMAT24_A_CR`: The number of correct responses from the Penn Progressive matrices; a measure of the subject's fluid intelligence.\n",
    " - `PSQI_Score`: The Pittsburgh Sleep Quality Index; a measure of the subject's quality of sleep.\n",
    "\n",
    "For more information on these variables, please visit [this link](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release). Here is a brief view of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the head of the covariates dataframe\n",
    "covariates.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Challenge**\n",
    "\n",
    "Your challenge is to fit a regression model to this data and generate confidence regions. Try the following model, generate CRs, and see if you can determine which regions of activation you have high confidence in.\n",
    "\n",
    "$$\\text{COPE}(s) = \\beta_0 + \\text{Sex} \\beta_1 + \\text{Age} \\beta_2 + \\text{PMAT24\\_A\\_CR} \\beta_3 + \\text{PSQI\\_SCORE} \\beta_4$$\n",
    "\n",
    "If you have any questions, or are unsure how to do this, please feel free to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's D Confidence Regions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above sections, we generated CRs for the set $\\mathcal{A}_c:=\\{s \\in S: \\mu(s) \\geq c\\}$. However, often in practice the units of measurement for $\\mu$ might not have a clear meaning. For instance, in fMRI %BOLD data is often subject to grand mean scaling; a preprocessing stage where all values in an image are rescaled to preserve a global mean. For this reason, it has been suggested that it is more interesting to instead estimate the set;\n",
    "\n",
    "$$\\mathcal{D}_c:=\\bigg\\{s \\in S: \\frac{\\mu(s)}{\\sigma(s)}\\geq c\\bigg\\}, \\quad \\text{using the estimate} \\quad \\hat{\\mathcal{D}}_c:=\\bigg\\{s \\in S: \\frac{\\hat{\\mu}(s)}{\\hat{\\sigma}(s)}\\geq c\\bigg\\}.$$\n",
    " \n",
    "As the units are standardised the above set retains the same interpretation, regardless of the units $\\mu$ was measured in. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some 3D simulated data to see what $\\mathcal{D}_c$ and $\\hat{\\mathcal{D}}_c$ look like. The below code will make some simple signal plus noise data of the form described in the [Background](#background) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subjects\n",
    "n = 30\n",
    "\n",
    "# Sigma/scalar standard devation\n",
    "sigma = 1\n",
    "\n",
    "# Mask file for background\n",
    "mask_file = os.path.join(os.getcwd(),'data','mask.nii.gz')\n",
    "\n",
    "# Output directory\n",
    "out_dir = os.path.join(os.getcwd(),'data','example_3D_cohens')\n",
    "\n",
    "# X matrix\n",
    "X = np.ones((n,1))\n",
    "\n",
    "# Output directory (feel free to change this to your desired output directory)\n",
    "out_dir = os.path.join(os.getcwd(),'data','example_3D_cohens')\n",
    "\n",
    "# Generate some test data\n",
    "data_files, beta_files, X = generate_data(n, 1, out_dir, sigma=sigma)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now compute the cohen's d image (stored in `d_file`), transformed residual images that are needed to generate cohen's d CRs (stored as `cohens_resid_files`), and the standard deviation of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Cohens residuals and cohens d\n",
    "dhat_file, cohens_resid_files, cohens_sigma_file = cohens(data_files, X, out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Developer's Note:** At the time of writing, full regression modelling is not supported by the `cohens` function. Instead the function currently automatically returns $\\hat{d}=\\frac{\\hat{\\beta}_0}{\\widehat{\\text{std}(\\hat{\\beta}_0)}}$. This means it can be used for \"signal plus noise\" models of the form described above, but more complicated contrasts of the form $L\\hat{\\beta}$ are not yet available."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image of the \"true\" Cohen's $d$ used to simulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out Cohen's d\n",
    "d = read_image(beta_files[0])/sigma\n",
    "\n",
    "# Display the true d image\n",
    "display_volume(d, bg=mask_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is image of our estimated $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated d image\n",
    "display_volume(dhat_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner to the CRs $\\hat{\\mathcal{A}}^{+}_c$ and $\\hat{\\mathcal{A}}^{-}_c$ we generated for $\\mathcal{A}_c$, we can generate confidence regions $\\hat{\\mathcal{D}}_c^{+}$ and $\\hat{\\mathcal{D}}_c^{-}$ that satisfy:\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}\\mathbb{P}[\\hat{\\mathcal{D}}_c^{+} \\subseteq \\mathcal{D}_c \\subseteq \\hat{\\mathcal{D}}_c^{-}]=1-\\alpha$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CR toolbox, we can do this by using the same functions as we used above. However, the mean image (`muhat_file`) **must** be replaced with the cohen's d file (`d_file`), the regression residuals used to generate the CRs in the previous sections (`resid_files`) **must** be replaced with the adjusted Cohen's d residuals we obtained above (`cohens_resid_files`), and the `type` argument **must** be set to `cohens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide a confidence level alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# Decide threshold c\n",
    "c = 0.5\n",
    "\n",
    "# Generate confidence regions (to save time, we will use 2000 bootstrap iterations for now)\n",
    "lower_cr_file, upper_cr_file, estimated_dc_file, quantile_estimate = generate_CRs(dhat_file, cohens_sigma_file, cohens_resid_files, out_dir, c, 1-alpha, n_boot=2000, mode='cohens')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets display the CRs we've generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the estimated mu\n",
    "print(quantile_estimate)\n",
    "display_crs(estimated_dc_file, upper_cr_file, lower_cr_file, mask_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Test your understanding:** You may find that you need many more subjects to get a good CR resemblence for $\\mathcal{D}_c$ than you did for $\\mathcal{A}_c$. Why do you think this might be? (Hint: Compare the estimated Cohen's $d$ image with the estimated mean in the previous sections.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math behind generating Cohen's $d$ CRs is very similar to the CR generation discussed above; theory shows that in order to generate Cohen's $d$ CR's we need only look at the distribution of the noise on the boundary of $\\mathcal{D}_c$, and we do this using a Wild $t-$ bootstrap. For further details, see [Bowring, et al. (2021)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7836238/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will delete the redundant files generated in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to remove the files from this section set this to True\n",
    "you_would_like = True\n",
    "\n",
    "# Delete files if you would like\n",
    "if you_would_like:\n",
    "\n",
    "    # List of files or file collections to remove\n",
    "    files_to_remove = [estimated_dc_file, upper_cr_file, lower_cr_file, dhat_file, cohens_resid_files, cohens_sigma_file, data_files, beta_files]\n",
    "\n",
    "    # Remove each file or file collection\n",
    "    for files in files_to_remove:\n",
    "        remove_files(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
